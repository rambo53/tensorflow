{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, activation, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activations_list = activations\n",
    "        self.weights_list = []\n",
    "        \n",
    "    # The build method will be called once\n",
    "    # we know the shape of the previous Layer: input_shape\n",
    "    def build(self, input_shape):\n",
    "        # Create trainable weights variables for this layer.\n",
    "        # We create matrix of weights for each layer\n",
    "        # Each weight have this shape: (previous_layer_size, layer_size)\n",
    "        # input shape = (none, 10) ici car on peut avoir n'importe quel nombre de vecteur mais ils ont 10 valeurs Ã  chaque fois\n",
    "        # pour le premier layer, puis ensuite shape = (none, 4)\n",
    "        i = 0\n",
    "        for units in self.units:\n",
    "            weights = self.add_weight(\n",
    "                        name=f\"weights-{i}\",\n",
    "                        shape=(input_shape[1], units),\n",
    "                        initializer='uniform',\n",
    "                        trainable=True\n",
    "            )\n",
    "            i += 1\n",
    "            self.weights_list.append(weights)\n",
    "            input_shape = (None, units)\n",
    "        super(MlpLayer, self).build(input_shape)\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        output = x\n",
    "\n",
    "        # We go through each weight to compute the dot product between the previous\n",
    "        # activation and the weight of the layer.\n",
    "        # At the first pass, the previous activation is just the variable \"x\": The input vector\n",
    "        for weights, activation in zip(self.weights_list, self.activations_list):\n",
    "            # We can still used low level operations as tf.matmul, tf.nn.relu... \n",
    "            output = tf.matmul(output, weights)\n",
    "\n",
    "            if activation == \"relu\":\n",
    "                output = tf.nn.relu(output)\n",
    "            elif activation == \"sigmoid\":\n",
    "                output = tf.nn.sigmoid(output)\n",
    "            elif activation == \"softmax\":\n",
    "                output = tf.nn.softmax(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(MyLayer([4,2], [\"relu\", \"softmax\"]))\n",
    "\n",
    "model.predict(np.zeros((5, 10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
